{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1JYcXO-92pDL-ybaznzwflcunrzzhbfTT",
      "authorship_tag": "ABX9TyNAPiLv2IRUUsj83nKbmhy6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nataliaespector/CEIA-VisionPorComputadoraII/blob/main/2_Eliminar_duplicados.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trabajo Final Visión por Computadora II - CEIA - UBA\n",
        "## Eliminar imágenes duplicadas\n",
        "### Dataset seleccionado: Chest CT-Scan images Dataset (Kaggle).\n",
        "\n",
        "Objetivo: Luego del EDA inicial, se observó que el dataset de 1000 imágenes contaba con 153 duplicados exactos. Al eliminarlos, se observó que algunos de los split quedaban con muy pocas imágenes. Por esta razón, se decidió unir el dataset, eliminar duplicados y volver a realizar el split en train, test y valid. Se genera un nuevo dataset *Data_Clean* que se utilizará para los entrenamientos posteriores."
      ],
      "metadata": {
        "id": "NRfh7QQhpxsA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importar librerías"
      ],
      "metadata": {
        "id": "-_JkZ7vjfLCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imagehash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ej6eDaxBfVaH",
        "outputId": "11a01602-b424-4105-93e3-e81f0e9c4e7c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imagehash in /usr/local/lib/python3.12/dist-packages (4.3.2)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.12/dist-packages (from imagehash) (1.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from imagehash) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from imagehash) (11.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from imagehash) (1.16.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import hashlib\n",
        "import random\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import shutil\n",
        "\n",
        "from PIL import Image\n",
        "import imagehash\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "OSoVEKVbq9SD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d291068e"
      },
      "source": [
        "## Carga de datos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leer carpeta cargada en Drive y verificar su contenido"
      ],
      "metadata": {
        "id": "LXMtNAqgfqKQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb8251b6",
        "outputId": "0b150828-4053-4979-f324-340f89653f77"
      },
      "source": [
        "data_dir = '/content/drive/MyDrive/Data'\n",
        "\n",
        "if os.path.exists(data_dir):\n",
        "    print(f\"Contenido de {data_dir}:\")\n",
        "    for item in os.listdir(data_dir):\n",
        "        print(item)\n",
        "else:\n",
        "    print(f\"El directorio {data_dir} no existe.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contenido de /content/drive/MyDrive/Data:\n",
            "train\n",
            "test\n",
            "valid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb113c48"
      },
      "source": [
        "Verificar los subdirectorios `train`, `test`, y `valid` y su contenido\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53517053",
        "outputId": "84c81c53-1414-46d0-c66c-d0c79b3d387b"
      },
      "source": [
        "for subdir in ['train', 'test', 'valid']:\n",
        "    subdir_path = os.path.join(data_dir, subdir)\n",
        "    if os.path.exists(subdir_path) and os.path.isdir(subdir_path):\n",
        "        print(f\"\\nContenido de {subdir_path}:\")\n",
        "        contents = os.listdir(subdir_path)\n",
        "        for i, item in enumerate(contents):\n",
        "            print(f\"  {item}\")\n",
        "    else:\n",
        "        print(f\"\\nEl directorio {subdir_path} no existe.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Contenido de /content/drive/MyDrive/Data/train:\n",
            "  adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib\n",
            "  large.cell.carcinoma_left.hilum_T2_N2_M0_IIIa\n",
            "  squamous.cell.carcinoma_left.hilum_T1_N2_M0_IIIa\n",
            "  normal\n",
            "\n",
            "Contenido de /content/drive/MyDrive/Data/test:\n",
            "  large.cell.carcinoma\n",
            "  normal\n",
            "  adenocarcinoma\n",
            "  squamous.cell.carcinoma\n",
            "\n",
            "Contenido de /content/drive/MyDrive/Data/valid:\n",
            "  adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib\n",
            "  large.cell.carcinoma_left.hilum_T2_N2_M0_IIIa\n",
            "  squamous.cell.carcinoma_left.hilum_T1_N2_M0_IIIa\n",
            "  normal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eliminar imágenes duplicadas"
      ],
      "metadata": {
        "id": "bWoUy1Kh2e35"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se utilizan funciones previamente utilizadas para el EDA para buscar las imágenes duplicadas mediante el hash MD5."
      ],
      "metadata": {
        "id": "0KsLAAlQgxkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directorio raíz del dataset original\n",
        "data_dir = Path(data_dir)\n",
        "\n",
        "# Extensiones de imagen válidas\n",
        "image_extensions = {'.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'}\n",
        "\n",
        "# Función para normalizar nombres de clases\n",
        "def normalize_class_name(class_name):\n",
        "    if class_name == 'normal':\n",
        "        return 'normal'\n",
        "    elif class_name.startswith('adenocarcinoma'):\n",
        "        return 'adenocarcinoma_left'\n",
        "    elif class_name.startswith('large.cell.carcinoma'):\n",
        "        return 'large.cell.carcinoma_left'\n",
        "    elif class_name.startswith('squamous.cell.carcinoma'):\n",
        "        return 'squamous.cell.carcinoma_left'\n",
        "    else:\n",
        "        return class_name\n",
        "\n",
        "# Función para calcular hash perceptual (dhash)\n",
        "def calculate_dhash(image_path):\n",
        "    \"\"\"Calcula el hash perceptual (dhash) de una imagen\"\"\"\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        # Convertir a RGB si es necesario\n",
        "        if img.mode != 'RGB':\n",
        "            img = img.convert('RGB')\n",
        "        # Redimensionar a 9x8 para dhash\n",
        "        img = img.resize((9, 8), Image.Resampling.LANCZOS)\n",
        "        # Convertir a escala de grises\n",
        "        img = img.convert('L')\n",
        "        # Calcular dhash\n",
        "        hash_value = imagehash.dhash(img)\n",
        "        return str(hash_value)\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "# Función para calcular hash MD5 (duplicados exactos)\n",
        "def calculate_md5(image_path):\n",
        "    \"\"\"Calcula el hash MD5 de un archivo\"\"\"\n",
        "    try:\n",
        "        with open(image_path, 'rb') as f:\n",
        "            return hashlib.md5(f.read()).hexdigest()\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "print(\"Buscando imágenes y calculando hashes...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "image_info = []\n",
        "\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    split_path = data_dir / split\n",
        "    if not split_path.exists():\n",
        "        continue\n",
        "\n",
        "    for class_folder in split_path.iterdir():\n",
        "        if not class_folder.is_dir():\n",
        "            continue\n",
        "\n",
        "        class_name = class_folder.name\n",
        "        normalized_class = normalize_class_name(class_name)\n",
        "\n",
        "        for image_file in class_folder.iterdir():\n",
        "            if image_file.is_file() and image_file.suffix in image_extensions:\n",
        "                md5 = calculate_md5(image_file)\n",
        "                dhash = calculate_dhash(image_file)\n",
        "\n",
        "                info = {\n",
        "                    'Split': split,\n",
        "                    'Clase': normalized_class,\n",
        "                    'Clase_original': class_name,\n",
        "                    'Archivo': image_file.name,\n",
        "                    'Ruta': str(image_file),\n",
        "                    'MD5': md5,\n",
        "                    'DHash': dhash,\n",
        "                }\n",
        "                image_info.append(info)\n",
        "\n",
        "# Pasar a DataFrame\n",
        "df = pd.DataFrame(image_info)\n",
        "print(\"Total de imágenes encontradas:\", len(df))\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6Ct_AmbThTi",
        "outputId": "e17f72f5-7a58-4849-c8c3-52838ce6386a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buscando imágenes y calculando hashes...\n",
            "================================================================================\n",
            "Total de imágenes encontradas: 1000\n",
            "   Split                Clase                              Clase_original  \\\n",
            "0  train  adenocarcinoma_left  adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib   \n",
            "1  train  adenocarcinoma_left  adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib   \n",
            "2  train  adenocarcinoma_left  adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib   \n",
            "3  train  adenocarcinoma_left  adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib   \n",
            "4  train  adenocarcinoma_left  adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib   \n",
            "\n",
            "          Archivo                                               Ruta  \\\n",
            "0  000015 (9).png  /content/drive/MyDrive/Data/train/adenocarcino...   \n",
            "1  000009 (7).png  /content/drive/MyDrive/Data/train/adenocarcino...   \n",
            "2  000021 (5).png  /content/drive/MyDrive/Data/train/adenocarcino...   \n",
            "3  000021 (4).png  /content/drive/MyDrive/Data/train/adenocarcino...   \n",
            "4  000015 (4).png  /content/drive/MyDrive/Data/train/adenocarcino...   \n",
            "\n",
            "                                MD5             DHash  \n",
            "0  f8f2b0a139bda40bc0ce9166e1a07a55  c7b2989a9ad6f4b8  \n",
            "1  626bd6363e427d7738d6241403ae6186  69d496b2939296c8  \n",
            "2  64b0644292f09556572aaf720953f469  ccb0c6969696cccc  \n",
            "3  45204fe06e9f5b185b48969aa769f1fe  cc98b43232b2f4f8  \n",
            "4  28fc90d52d8d2bc399fb205087da0f9d  ccb0b0ba9a96d479  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar filas sin MD5 (si hubo errores de lectura)\n",
        "df_clean = df.dropna(subset=['MD5']).copy()\n",
        "\n",
        "# Cantidad por clase antes de deduplicar\n",
        "print(\"\\nImágenes por clase ANTES de deduplicar:\")\n",
        "print(df_clean['Clase'].value_counts())\n",
        "\n",
        "# Mantener una sola imagen por MD5 (la primera)\n",
        "df_dedup = df_clean.sort_values('Ruta').drop_duplicates(subset=['MD5'], keep='first')\n",
        "\n",
        "print(\"\\nImágenes por clase DESPUÉS de deduplicar (MD5):\")\n",
        "print(df_dedup['Clase'].value_counts())\n",
        "\n",
        "print(\"\\nTotal antes:\", len(df_clean), \" | Total después de deduplicar:\", len(df_dedup))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUqzNSFHX4GU",
        "outputId": "4ba1bfc2-d39d-461d-d58d-0664322cacc6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Imágenes por clase ANTES de deduplicar:\n",
            "Clase\n",
            "adenocarcinoma_left             338\n",
            "squamous.cell.carcinoma_left    260\n",
            "normal                          215\n",
            "large.cell.carcinoma_left       187\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Imágenes por clase DESPUÉS de deduplicar (MD5):\n",
            "Clase\n",
            "adenocarcinoma_left             337\n",
            "squamous.cell.carcinoma_left    257\n",
            "large.cell.carcinoma_left       187\n",
            "normal                           66\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Total antes: 1000  | Total después de deduplicar: 847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nuevo split\n",
        "\n",
        "Se realiza nuevamente la separación entre train, test y valid con el df sin duplicados"
      ],
      "metadata": {
        "id": "o5YOfpc4i024"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_size = 0.15\n",
        "valid_size = 0.15\n",
        "random_state = 42\n",
        "\n",
        "# Separar test\n",
        "df_rest, df_test = train_test_split(\n",
        "    df_dedup,\n",
        "    test_size=test_size,\n",
        "    stratify=df_dedup['Clase'],\n",
        "    random_state=random_state\n",
        ")\n",
        "\n",
        "# Separar train y valid\n",
        "valid_ratio = valid_size / (1 - test_size)\n",
        "\n",
        "df_train, df_valid = train_test_split(\n",
        "    df_rest,\n",
        "    test_size=valid_ratio,\n",
        "    stratify=df_rest['Clase'],\n",
        "    random_state=random_state\n",
        ")\n",
        "\n",
        "print(\"\\nTamaños finales:\")\n",
        "print(\"Train:\", len(df_train))\n",
        "print(\"Valid:\", len(df_valid))\n",
        "print(\"Test: \", len(df_test))\n",
        "\n",
        "print(\"\\nClases en TRAIN:\")\n",
        "print(df_train['Clase'].value_counts())\n",
        "\n",
        "print(\"\\nClases en VALID:\")\n",
        "print(df_valid['Clase'].value_counts())\n",
        "\n",
        "print(\"\\nClases en TEST:\")\n",
        "print(df_test['Clase'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDKxyiOyYm0t",
        "outputId": "1d783fbb-d988-4ce0-bcaa-5258aaf46f87"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tamaños finales:\n",
            "Train: 592\n",
            "Valid: 127\n",
            "Test:  128\n",
            "\n",
            "Clases en TRAIN:\n",
            "Clase\n",
            "adenocarcinoma_left             235\n",
            "squamous.cell.carcinoma_left    180\n",
            "large.cell.carcinoma_left       131\n",
            "normal                           46\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Clases en VALID:\n",
            "Clase\n",
            "adenocarcinoma_left             51\n",
            "squamous.cell.carcinoma_left    38\n",
            "large.cell.carcinoma_left       28\n",
            "normal                          10\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Clases en TEST:\n",
            "Clase\n",
            "adenocarcinoma_left             51\n",
            "squamous.cell.carcinoma_left    39\n",
            "large.cell.carcinoma_left       28\n",
            "normal                          10\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generar nueva carpeta Data_Clean\n",
        "\n",
        "Se crea el nuevo directorio con los datos sin duplicados y separados según el nuevo split"
      ],
      "metadata": {
        "id": "3brH81orjeb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nuevo directorio Data_Clean\n",
        "new_root = data_dir.parent / \"Data_Clean\"\n",
        "\n",
        "def copiar_split(df_split, split_name, root_dest):\n",
        "    for _, row in df_split.iterrows():\n",
        "        src = Path(row['Ruta'])\n",
        "        clase = row['Clase']\n",
        "        dest_dir = root_dest / split_name / clase\n",
        "        dest_dir.mkdir(parents=True, exist_ok=True)\n",
        "        dest = dest_dir / src.name\n",
        "\n",
        "        if dest.exists():\n",
        "            continue\n",
        "\n",
        "        shutil.copy2(src, dest)\n",
        "\n",
        "    print(f\"Copiadas {len(df_split)} imágenes a {split_name}/\")\n",
        "\n",
        "# Crear nueva estructura limpia\n",
        "copiar_split(df_train, 'train', new_root)\n",
        "copiar_split(df_valid, 'valid', new_root)\n",
        "copiar_split(df_test,  'test',  new_root)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw9gx7ZJZKa4",
        "outputId": "d1975057-11e5-453a-8c0e-dd00f677c737"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copiadas 592 imágenes a train/\n",
            "Copiadas 127 imágenes a valid/\n",
            "Copiadas 128 imágenes a test/\n"
          ]
        }
      ]
    }
  ]
}